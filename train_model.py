# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v8Hm0pBVQIZNaFiTjUu_XpzJbSnTggM9
"""

!pip install pandas numpy scikit-learn xgboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder

train_file = "train.csv"
test_file = "test.csv"
submission_file = "water_consumption_predictions.csv"

try:
    train_data = pd.read_csv(train_file)
    test_data = pd.read_csv(test_file)
except FileNotFoundError:
    print("Error: One or more dataset files not found. Make sure 'train.csv' and 'test.csv' are in the correct folder.")
    exit()

required_columns = [
    "Timestamp", "Residents", "Apartment_Type", "Temperature", "Humidity", "Water_Price",
    "Period_Consumption_Index", "Income_Level", "Guests", "Amenities", "Appliance_Usage", "Water_Consumption"
]

missing_cols = [col for col in required_columns if col not in train_data.columns]
if missing_cols:
    print(f"Error: Missing columns in train.csv - {missing_cols}")
    exit()

train_data["Timestamp"] = pd.to_datetime(train_data["Timestamp"], format='%d/%m/%Y %H') # Changed the format string to match the actual data format: day/month/year hour
test_data["Timestamp"] = pd.to_datetime(test_data["Timestamp"], format='%d/%m/%Y %H')  # Changed the format string to match the actual data format: day/month/year hour

for df in [train_data, test_data]:
    df["Hour"] = df["Timestamp"].dt.hour
    df["DayOfWeek"] = df["Timestamp"].dt.dayofweek
    df["Month"] = df["Timestamp"].dt.month

categorical_columns = ["Apartment_Type", "Income_Level", "Amenities"]
label_encoders = {}

for col in categorical_columns:
    # Create a LabelEncoder for each column
    le = LabelEncoder()

    # Fit on combined unique values from both train and test data
    all_values = pd.concat([train_data[col], test_data[col]]).astype(str).unique()
    le.fit(all_values)

    # Transform both train and test data
    train_data[col] = le.transform(train_data[col].astype(str))
    test_data[col] = le.transform(test_data[col].astype(str))

    # Store the encoder for this column
    label_encoders[col] = le

features = [
    "Residents", "Temperature", "Humidity", "Water_Price", "Period_Consumption_Index",
    "Income_Level", "Guests", "Apartment_Type", "Hour", "DayOfWeek", "Month", "Appliance_Usage"
]

target = "Water_Consumption"

if train_data[features].isnull().sum().sum() > 0 or train_data[target].isnull().sum() > 0:
    print("Warning: Missing values detected. Consider handling missing data before training.")

X = train_data[features]
y = train_data[target]

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Train XGBoost model
model = xgb.XGBRegressor(
    objective="reg:squarederror",
    n_estimators=200,
    max_depth=6,
    learning_rate=0.05,
    random_state=42
)

# Train XGBoost model using the DMatrix data format for better integration with XGBoost's core functions
print("Training model...")

# Convert 'Humidity' to numeric, handling potential errors
X_train['Humidity'] = pd.to_numeric(X_train['Humidity'], errors='coerce')
X_valid['Humidity'] = pd.to_numeric(X_valid['Humidity'], errors='coerce')

# Fill any missing values after conversion (e.g., with the mean)
X_train['Humidity'].fillna(X_train['Humidity'].mean(), inplace=True)
X_valid['Humidity'].fillna(X_valid['Humidity'].mean(), inplace=True)

# Convert data to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True) # enable_categorical=True added
dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True) # enable_categorical=True added

# Specify parameters
params = {
    'objective': 'reg:squarederror',
    'n_estimators': 200,
    'max_depth': 6,
    'learning_rate': 0.05,
    'random_state': 42
}

# Train the model with early stopping
model = xgb.train(
    params,
    dtrain,
    evals=[(dvalid, 'eval')],  # Evaluation set
    early_stopping_rounds=10,  # Early stopping criteria
    verbose_eval=True  # Print evaluation metrics during training
)

# Predict on test data
if set(features).issubset(test_data.columns):
    # Convert 'Humidity' to numeric for test data as well
    test_data['Humidity'] = pd.to_numeric(test_data['Humidity'], errors='coerce')
    # Fill any missing values after conversion (e.g., with the mean)
    test_data['Humidity'].fillna(test_data['Humidity'].mean(), inplace=True)
    # Convert test data to DMatrix format
    dtest = xgb.DMatrix(test_data[features], enable_categorical=True)
    predictions = model.predict(dtest)
else:
    print("Error: Some feature columns are missing in test.csv. Please check your dataset.")
    exit()

# Prepare submission file
submission = pd.DataFrame({"Timestamp": test_data["Timestamp"], "Water_Consumption": predictions})
submission.set_index("Timestamp", inplace=True)

submission.to_csv(submission_file)
print(f"Prediction file created successfully: {submission_file}")